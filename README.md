The following are the required steps to get this LeNet model running on the CLAHE Sorghum-100 Dataset.
It should be noted that these instructions are for a Windows environment.

This repo contains 6 files:
- 2 CNNs written in python for use on the Sorghum-100 dataset
  - The first, LeNet_Baseline.py, is a baseline LeNet model with image augmentation.
  - The second, LeNet_Baseline_Ablation.py, is an ablation of the LeNet baseline, that removes image augmentation.
- train_cultivar_mapping.csv, a version of the dataset CSV mapping images to their label/class which I've altered to remove
  errors introduced by the person who made the CLAHE-processed dataset from the Sorghum-100 dataset, and to work with my implementation.
- 10_class_clahe_cultivar_map.csv, a subset of 10 classes from that image:label map.
- requirements.txt, a file generated by the pipreqs utility to quickly install the required dependencies
- this README.md file


Dependencies:
- Appropriate CPU/GPU, like a modern NVidia graphics card, for tensorflow calculations
- 16 GB of RAM
- Python 3.11 or later
- Latest version of PIP
- pipreqs
- keras 2.12.0
- matplotlib 3.7.1
- numpy 1.23.2
- pandas 1.5.3
- scikit_learn 1.2.2
- tensorflow 2.12.0


Installing Dependencies:
- The list of dependencies, generated by pipreqs, can be installed by navigating to the directory containing the requirements.txt file
  (it's the same directory that contains LeNet_Baseline.py files) and running the command "pip install -r requirements.txt"


Dataset and Sub-Dataset:
- The CLAHE Sorghum-100 dataset can be downloaded from the following URL: https://www.kaggle.com/datasets/joonasyoon/sorghum-fgvc9-clahe-512
- After downloading the zip file containing the dataset from the URL, unzip it and place the "train_images" folder into the directory
  containing LeNet_Baseline.py and requirements.txt
- Ensure that the 10_class_clahe_cultivar_map.csv file is in the same directory, and run get_10_class_images.py
  This will copy all the images from the 10 largest classes and paste them into a folder called "10_classes", creating the sub-dataset


Running The Model:
- Opening the folder in PyCharm or VSCode, proceed to open either LeNet_Baseline.py or LeNet_Baseline_Ablation.py, and run it from inside the IDE
  This will show the loss, accuracy, and precision for both the training and validation sets with each epoch as the model trains, and will also
  show you the MathPlotLib graph of each after all Epochs are complete
- Alternatively, run the python file of the model you wish to train from the terminal. This will still show the loss, accuracy, and precision,
  but will not show the graph of each over the course of all training epochs.
- Regardless of which option you use, after training is complete the graph will be output to "training_validation_metrics.png",
  and the predicted class/label for each image is recorded in the predicted_classes.csv


Changing The Training/Validation Dataset:
- Note that at this time I have not yet implemented batching in my model, and so attempting to train either the Baseline LeNet or the LeNet Ablation model
  on the full "train_images" dataset results in memory error, and is not yet possible
- To change the training/validation dataset from the 10-class dataset to the full dataset, navigate to line 61 in both the Baseline and Ablation models'
  python files. Un-comment lines 61 and 62, and comment-out lines 57 and 58
- Then proceed to line 96/97 and change the "num_classes" to 100


Changing Hyperparameters:
- The number of epochs can be changed on lines 135/143
